{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac130ccfbf2b47f69e65341c36f48d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_24522fe346ca462995653f97118df9aa",
              "IPY_MODEL_6e8990599c084c35a8fd198c8a7a0320",
              "IPY_MODEL_aae16c2d5fd74a77a5287c93c73f6d11"
            ],
            "layout": "IPY_MODEL_cc08239502ee427b9673c0a3b6d234a4"
          }
        },
        "24522fe346ca462995653f97118df9aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2404f4c666a4381a763088ee7262ee2",
            "placeholder": "​",
            "style": "IPY_MODEL_80fa97af396c4102a4c2fc08ce24f75f",
            "value": "  0%"
          }
        },
        "6e8990599c084c35a8fd198c8a7a0320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7280bb1175cd416da64287b9aec38fb1",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cf3f486b0e742ac95bf9b1279951956",
            "value": 0
          }
        },
        "aae16c2d5fd74a77a5287c93c73f6d11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_943de169237f48fe94b6bf0235fa4273",
            "placeholder": "​",
            "style": "IPY_MODEL_b812feb3866e47a08c03f4f60fb0bba6",
            "value": " 0/30 [00:00&lt;?, ?it/s]"
          }
        },
        "cc08239502ee427b9673c0a3b6d234a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2404f4c666a4381a763088ee7262ee2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80fa97af396c4102a4c2fc08ce24f75f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7280bb1175cd416da64287b9aec38fb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cf3f486b0e742ac95bf9b1279951956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "943de169237f48fe94b6bf0235fa4273": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b812feb3866e47a08c03f4f60fb0bba6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**13장. 생성 모델 (generative model)**\n",
        "\n",
        ": 주어진 데이터를 학습하여, 데이터 분포를 따르는 유사한 데이터를 생성하는 모델\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "22GtIXQObVfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.2.1 오토인코더란**\n",
        "\n",
        ": 입력을 출력으로 복사하는 신경망, 은닉층(병목층)이 입력층보다 작은 게 특징\n",
        "\n",
        "- 인코더 : 인지 네트워크. 특성 학습\n",
        "- 은닉층(병목층) : 데이터 압축\n",
        "- 디코더 : 생성 네트워크. 병목층에서 압축된 데이터를 재구성. 최대한 입력에 가까운 출력 생성\n",
        "- 손실함수는 인코더와 디코더의 차이로 계산\n",
        "\n",
        "오토인코더가 중요한 이유 :\n",
        "1. 데이터 압축 : 파일의 중요 특성만 압축 -> 용량이 작고 품질이 좋아져, 메모리 아낌\n",
        "2. 차원의 저주 예방 : 차원의 저주란 차원이 증가하면서 학습데이터 수가 차원 수보다 적어져서 성능이 저하되는 현상.\n",
        "  \n",
        "  오토인코더는 특성 개수를 줄여줌 -> 데이터 차원 감소 -> 차원의 저주 예방\n",
        "\n",
        "3. 특겅 추출 : 오토인코더는 비지도 학습 -> 자동으로 중요 특성 찾아줌"
      ],
      "metadata": {
        "id": "wIhYrPkM38CL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aSZAnKbybP_0"
      },
      "outputs": [],
      "source": [
        "#오토인코더 예제\n",
        "\n",
        "#라이브러리 호출\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#MNIST 데이터세트 내려받고 전처리\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='/content/drive/MyDrive/pytorch_ex/chap13/data', train=True, transform=transform, download=True\n",
        ")\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root='/content/drive/MyDrive/pytorch_ex/chap13/data', train=False, transform=transform, download=True\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-KGkTyCjXhR",
        "outputId": "fa2328a8-2f4f-40c7-be28-dd8e49b77fcf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#인코더, 디코더 구현하기\n",
        "\n",
        "##1. 인코더\n",
        "class Encoder(nn.Module) :\n",
        "\n",
        "  def __init__(self, encoded_space_dim, fc2_input_dim) :\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_cnn = nn.Sequential(  #이미지 데이터 처리를 위하여 합성곱 신경망 이용\n",
        "        nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.flatten = nn.Flatten(start_dim=1) #완전연결층\n",
        "    self.encoder_lin = nn.Sequential(  #출력 계층\n",
        "        nn.Linear(3 * 3 * 32, 128),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(128, encoded_space_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x) :\n",
        "    x = self.encoder_cnn(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.encoder_lin(x)\n",
        "    return x\n",
        "\n",
        "#####\n",
        "\n",
        "##2. 디코더\n",
        "class Decoder(nn.Module) :\n",
        "  def __init__(self, encoded_space_dim, fc2_input_dim) :\n",
        "    super().__init__()\n",
        "    self.decoder_lin = nn.Sequential(     #인코더의 출력을 디코더의 입력으로 사용하는 층\n",
        "        nn.Linear(encoded_space_dim, 128),\n",
        "        nn.ReLU(True),\n",
        "        nn.Linear(128, 3 * 3 * 32),\n",
        "        nn.ReLU(True)\n",
        "    )\n",
        "\n",
        "    self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3)) #인코더의 완전연결층과 같은 차원,크기로 펼침\n",
        "    self.decoder_conv = nn.Sequential(    #인코더의 합성곱층과 같게 하여 입출력 크기 같게 함\n",
        "        nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x) :\n",
        "    x = self.decoder_lin(x)\n",
        "    x = self.unflatten(x)\n",
        "    x = self.decoder_conv(x)\n",
        "    x = torch.sigmoid(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "ETVe4H4qlSV3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#인코더와 디코더 객체 초기화(정의)\n",
        "encoder = Encoder(encoded_space_dim=4, fc2_input_dim=128)\n",
        "decoder = Decoder(encoded_space_dim=4, fc2_input_dim=128)\n",
        "encoder.to(device)\n",
        "decoder.to(device)\n",
        "\n",
        "#손실함수, 옵티마이저 정의\n",
        "params_to_optimize = [\n",
        "    {'params': encoder.parameters()},\n",
        "    {'params': decoder.parameters()}\n",
        "    ] #인코더와 디코더에서 사용할 파라미터들을 다르게 지정\n",
        "optim = torch.optim.Adam(params_to_optimize, lr=0.001, weight_decay=1e-05)\n",
        "loss_fn = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "vKV9Si_68U09"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder)\n",
        "print('++++++'*4)\n",
        "print(decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzkAw5puM0hl",
        "outputId": "04ab2849-e4a3-43fc-f049-811ff94c938d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder(\n",
            "  (encoder_cnn): Sequential(\n",
            "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (6): ReLU(inplace=True)\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (encoder_lin): Sequential(\n",
            "    (0): Linear(in_features=288, out_features=128, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=128, out_features=4, bias=True)\n",
            "  )\n",
            ")\n",
            "++++++++++++++++++++++++\n",
            "Decoder(\n",
            "  (decoder_lin): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=128, out_features=288, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "  )\n",
            "  (unflatten): Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
            "  (decoder_conv): Sequential(\n",
            "    (0): ConvTranspose2d(32, 16, kernel_size=(3, 3), stride=(2, 2))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(8, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습 함수 생성\n",
        "def train_epoch(encoder, decoder, device, dataloader, loss_fn, optimizer, noise_factor=0.3) :\n",
        "  encoder.train() #인코더 훈련\n",
        "  decoder.train() #디코더 훈련\n",
        "  train_loss = [] #손실 저장 리스트 마련\n",
        "  for image_batch, _ in dataloader : #비지도 학습이므로 레이블은 버림\n",
        "    image_noisy = add_noise(image_batch, noise_factor)\n",
        "    image_noisy = image_noisy.to(device)\n",
        "    encoded_data = encoder(image_noisy)\n",
        "    decoded_data = decoder(encoded_data)\n",
        "    loss = loss_fn(decoded_data, image_noisy)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss.append(loss.detach().cpu().numpy())\n",
        "  return np.mean(train_loss)"
      ],
      "metadata": {
        "id": "0TUSxSgu9Jzm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 테스트 함수 생성\n",
        "def test_epoch(encoder, decoder, device, dataloader, loss_fn, noise_factor=0.3) :\n",
        "  encoder.eval()\n",
        "  decoder.eval()\n",
        "  with torch.no_grad() :\n",
        "    conc_out = [] #각 배치의 출력 저장 리스트 마련\n",
        "    conc_label = []\n",
        "    for image_batch, _ in dataloader :\n",
        "      image_batch = image_batch.to(device)\n",
        "      encoded_data = encoder(image_batch)\n",
        "      decoded_data = decoder(encoded_data)\n",
        "      conc_out.append(decoded_data.cpu())\n",
        "      conc_label.append(image_batch.cpu())\n",
        "    conc_out = torch.cat(conc_out) #리스트 형태로 저장된 모든 값 -> 하나의 텐서\n",
        "    conc_label = torch.cat(conc_label)\n",
        "    val_loss = loss_fn(conc_out, conc_label) #손실함수를 이용해 오차 계산\n",
        "  return val_loss.data"
      ],
      "metadata": {
        "id": "PPiBytyy-s5C"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#입력 데이터에 추가할 노이즈 생성 함수\n",
        "def add_noise(inputs, noise_factor=0.3) :\n",
        "  noisy = inputs + torch.randn_like(inputs) * noise_factor # torch.randn_like() : 입력과 동일한 크기의 노이즈 텐서 생서\n",
        "  noisy = torch.clip(noisy, 0., 1.) #데이터 값의 범위 조정 (최소0 ~ 최대1)\n",
        "  return noisy"
      ],
      "metadata": {
        "id": "ixgO7QAWB6ng"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#이미지 시각화 함수 : 에포크가 진행될수록 노이즈 데이터로 새로운 이미지가 어떻게 만들어지는지~\n",
        "\n",
        "def plot_ae_outputs(encoder, decoder, n=5, noise_factor=0.3) :\n",
        "  plt.figure(figsize=(10,4.5))\n",
        "  for i in range(n) :\n",
        "    ax = plt.subplot(3, n, i+1) #열, 행, 인덱스. 즉 3x5 이미지 출력\n",
        "    img = test_dataset[i][0].unsqueeze(0)\n",
        "    image_noisy = add_noise(img, noise_factor)\n",
        "    iamge_noisy = image_noisy.to(device)\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    with torch.no_grad() :\n",
        "      rec_img = decoder(encoder(image_noisy))\n",
        "\n",
        "    plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray') #테스트 데이터셋 출력\n",
        "    ax.get_xaxis().set_visible(False) #그래프 눈금 표시x\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    if i == n//2 :\n",
        "      ax.set_title('원래 이미지')\n",
        "    ax = plt.subplot(3, n, i + 1 + n)\n",
        "    plt.imshow(image_noisy.cpu().squeeze().numpy(), cmap='gist_gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    if i == n//2 :\n",
        "      ax.set_title('노이즈가 적용되어 손상된 이미지')\n",
        "\n",
        "    ax = plt.subplot(3, n, i + 1 + n + n)\n",
        "    plt.imshow(rec_img.cpu().squeeze().numpy, cmap='gist_gray')\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    if i == n//2 :\n",
        "      ax.set_title('노이즈 데이터를 이용하여 재구성된 이미지')\n",
        "  plt.subplots_adjust(left=0.1, bottom=0.1, right=0.7, top=0.9, wspace=0.3, hspace=0.3)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "jQGVOGlXDqnc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습\n",
        "import numpy as np\n",
        "\n",
        "num_epochs = 30\n",
        "history_da = {'train_loss':[], 'val_loss':[]}\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "for epoch in range(num_epochs) :\n",
        "  print(f'EPOCH {epoch+1}/{num_epochs}')\n",
        "  train_loss = train_epoch(encoder=encoder, decoder=decoder, device=device, dataloader=train_dataloader,\n",
        "                           loss_fn=loss_fn, optimizer=optim, noise_factor=0.3)\n",
        "  val_loss = test_epoch(encoder=encoder, decoder=decoder, device=device, dataloader=test_dataloader,\n",
        "                        loss_fn=loss_fn, noise_factor=0.3)\n",
        "  history_da['train_loss'].append(train_loss)\n",
        "  history_da['val_loss'].append(val_loss)\n",
        "  print(f'\\n Epoch {epoch+1}/{num_epochs} \\t train loss {train_loss:.3f} \\t val loss {val_loss:.3f}')\n",
        "  plot_ae_outputs(encoder, decoder, noise_factor=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "_fxht0reJgHk",
        "outputId": "481e7906-7b58-4693-d65c-024c02106c3a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1/30\n",
            "\n",
            " Epoch 1/30 \t train loss 0.052 \t val loss 0.042\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Image data of dtype object cannot be converted to float",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-56075e112072>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mhistory_da\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\n Epoch {epoch+1}/{num_epochs} \\t train loss {train_loss:.3f} \\t val loss {val_loss:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m   \u001b[0mplot_ae_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-28-a5bff638748d>\u001b[0m in \u001b[0;36mplot_ae_outputs\u001b[0;34m(encoder, decoder, n, noise_factor)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gist_gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_yaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3590\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3591\u001b[0m ) -> AxesImage:\n\u001b[0;32m-> 3592\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   3593\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3594\u001b[0m         \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m             return func(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                 \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5943\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_aspect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5945\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5946\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Needed e.g. to apply png palette.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_normalize_image_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_imcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_masked_invalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcan_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"same_kind\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m             raise TypeError(f\"Image data of dtype {A.dtype} cannot be \"\n\u001b[0m\u001b[1;32m    639\u001b[0m                             f\"converted to float\")\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x450 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13.2.2 변형 오토인코더**"
      ],
      "metadata": {
        "id": "dqUES-iiPdrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorboardX"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zXZsgBsMZ1B",
        "outputId": "6716674e-a944-4401-f204-c8e837d39547"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (5.29.5)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#필요한 라이브러리 호출\n",
        "import datetime\n",
        "import os\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "pvcbC7ypUsxf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋 내려받고, 이미지->텐서 변환\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='/content/drive/MyDrive/pytorch_ex/chap13/data',\n",
        "                               train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='/content/drive/MyDrive/pytorch_ex/chap13/data',\n",
        "                              train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=4, pin_memory=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_isCGEbVQ9s",
        "outputId": "741459df-d2e9-480c-9fe2-59c9c84dd7fe"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#인코더 네트워크 생성 : 데이터가 주어졌을 때 디코더가 원래 데이터로 잘 복원할 수 있는 이상적인 확률 분포 찾음(<->오토인코더: 그냥 정해져있는 분포 사용)\n",
        "class Encoder(nn.Module) :\n",
        "  def __init__(self, input_dim, hidden_dim, latent_dim) :\n",
        "    super(Encoder, self).__init__()\n",
        "    self.input1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.input2 = nn.Linear(hidden_dim, latent_dim)\n",
        "    self.mean = nn.Linear(hidden_dim, latent_dim) #평균\n",
        "    self.var = nn.Linear(hidden_dim, latent_dim) #분산\n",
        "\n",
        "    self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "    self.training = True\n",
        "\n",
        "  def forward(self, x) :\n",
        "    h_ = self.LeakyReLU(self.input1(x))\n",
        "    h_ = self.LeakyReLU(self.input2(h_))\n",
        "    mean = self.mean(h_)\n",
        "    log_var = self.var(h_)\n",
        "    return mean, log_var #인코더에서 평균과 분산 반환"
      ],
      "metadata": {
        "id": "bUBhUArKWou-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#디코더 네트워크 : 추출한 샘플을 입력으로 받아서 다시 원본으로 재구축(재생성)\n",
        "class Decoder(nn.Module) :\n",
        "  def __init__(self, latent_dim, hidden_dim, output_dim) :\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden1 = nn.Linear(latent_dim, hidden_dim)\n",
        "    self.hidden2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.output = nn.Linear(hidden_dim, output_dim)\n",
        "    self.LeakyReLU = nn.LeakyReLU(0.2)\n",
        "\n",
        "  def forward(self, x) :\n",
        "    h = self.LeakyReLU(self.hidden1(x))\n",
        "    h = self.LeakyReLU(self.hidden2(x))\n",
        "    x_hat = torch.sigmoid(self.output(h))\n",
        "    return x_hat"
      ],
      "metadata": {
        "id": "gPel5cL7boQB"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#변형 오토인코더 네트워크\n",
        "class Model(nn.Module) :\n",
        "  def __init__(self, Encoder, Decoder) :\n",
        "    super(Model, self).__init__()\n",
        "    self.Encoder = Encoder\n",
        "    self.Decoder = Decoder\n",
        "\n",
        "  def reparameterization(self, mean, var) : #평균과 표준편차가 주어졌을 떄 잠재벡터(z)를 만드는 함수\n",
        "    epsilon = torch.randn_like(var).to(device)\n",
        "    z = mean + var * epsilon\n",
        "    return z\n",
        "\n",
        "  def forward(self, x) :\n",
        "    mean, log_var = self.Encoder(x)\n",
        "    z = self.reparameterization(mean, torch.exp(0.5 * log_var))\n",
        "    x_hat = self.Decoder(z)\n",
        "    return x_hat, mean, log_var #디코더 결과, 평균, 표준편차 반환"
      ],
      "metadata": {
        "id": "XXQNN9aId1QJ"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#인코더와 디코더 객체 초기화\n",
        "x_dim = 784\n",
        "hidden_dim = 400\n",
        "latent_dim = 200\n",
        "epochs = 30\n",
        "batch_size = 100\n",
        "\n",
        "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
        "decoder = Decoder(latent_dim=latent_dim, hidden_dim=hidden_dim, output_dim=x_dim)\n",
        "\n",
        "model = Model(Encoder=encoder, Decoder=decoder).to(device)"
      ],
      "metadata": {
        "id": "j6ujWxJHfbHN"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#오차 계산 위한 손실 함수 정의\n",
        "def loss_function(x, x_hat, mean, log_var) :\n",
        "  reproduction_loss = nn.functional.binary_cross_entropy(x_hat, x, reduction='sum')\n",
        "  KLD = -0.5 * torch.sum(1 + log_var - mean_pow(2) - log_var.exp())\n",
        "  return reproduction_loss, KLD\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "ZzcZ-lAff1C9"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습 함수 정의\n",
        "saved_loc = 'scalar/'\n",
        "writer = SummaryWriter(saved_loc)\n",
        "\n",
        "model.train()\n",
        "\n",
        "def train(epoch, model, train_loader, optimizer) :\n",
        "  train_loss = 0\n",
        "  for vatch_idx, (x, _) in enumerate(train_loader) :\n",
        "    x = x.view(batch_size, x_dim)\n",
        "    x = x.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    x_hat, mean, log_var = model(x)\n",
        "    BCE, KLD = loss_function(x, x_hat, mean, log_var)\n",
        "    loss = BCE + KLD\n",
        "    writer.add_scalar( 'Train/Reconstruction Error', BCE.item(),\n",
        "                       batch_idx + epoch * (len(train_loader.dataset)/batch_size) )\n",
        "    writer.add_scalar( 'Train/KL-Divergence', KLD.item(),\n",
        "                       batch_idx + epoch * (len(train_loader.dataset)/batch_size) )\n",
        "    writer.add_scalar( 'Train/Total Loss', loss.item(),\n",
        "                       batch_idx + epoch * (len(train_loader.dataset)/batch_size) )\n",
        "\n",
        "    train_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx % 100 == 0 :\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(epoch, batch_idx * len(x), (train_loader.dataset), 100. * batch_idx / len(train_loader)))\n",
        "\n",
        "  print(f'=====> Epoch: {epoch} Average loss: {train_los / len(train_loader.dataset):.4f}')"
      ],
      "metadata": {
        "id": "G2p-KTOCgryW"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 평가 함수 정의\n",
        "\n",
        "def test(epoch, model, test_loader) :\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  with torch.no_grad() :\n",
        "    for batch_idx, (x, _) in enumerate(test_loader) :\n",
        "      x = x.view(batch_size, x_dim)\n",
        "      x = x.to(device)\n",
        "      x_hat, mean, log_var = model(x)\n",
        "      BCE, KLD = loss_function(x, x_hat, mean, log_var)\n",
        "      loss = BCE + KLD\n",
        "\n",
        "    writer.add_scalar( 'Test/Reconstruction Error', BCE.item(),\n",
        "                       batch_idx + epoch * (len(test_loader.dataset)/batch_size) )\n",
        "    writer.add_scalar( 'Test/KL-Divergence', KLD.item(),\n",
        "                       batch_idx + epoch * (len(test_loader.dataset)/batch_size) )\n",
        "    writer.add_scalar( 'Test/Total Loss', loss.item(),\n",
        "                       batch_idx + epoch * (len(test_loader.dataset)/batch_size) )\n",
        "    test_loss += loss.item()\n",
        "\n",
        "    if batch_idx == 0 :\n",
        "      n = min(x.size(0), 8)\n",
        "      comparison = torch.cat([x[:n], x_hat.view(batch_size, x_dim)[:n]])\n",
        "      grid = torchvision.utils.make_grid(comparison.cpu())\n",
        "      writer.add_image('Test image - Above: Real data, below: reconstruction data', grid, epoch)"
      ],
      "metadata": {
        "id": "tfmDhxi5jTau"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습\n",
        "from tqdm.auto import tqdm\n",
        "for epoch in tqdm(range(0, epochs)) :\n",
        "  train(epoch, model, train_loader, optimizer)\n",
        "  test(epoch, model, test_loader)\n",
        "  print('\\n')\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460,
          "referenced_widgets": [
            "ac130ccfbf2b47f69e65341c36f48d0d",
            "24522fe346ca462995653f97118df9aa",
            "6e8990599c084c35a8fd198c8a7a0320",
            "aae16c2d5fd74a77a5287c93c73f6d11",
            "cc08239502ee427b9673c0a3b6d234a4",
            "b2404f4c666a4381a763088ee7262ee2",
            "80fa97af396c4102a4c2fc08ce24f75f",
            "7280bb1175cd416da64287b9aec38fb1",
            "9cf3f486b0e742ac95bf9b1279951956",
            "943de169237f48fe94b6bf0235fa4273",
            "b812feb3866e47a08c03f4f60fb0bba6"
          ]
        },
        "id": "dmAk8F94n4T2",
        "outputId": "5752dfc0-c8dd-49e6-a544-31ca94034735"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac130ccfbf2b47f69e65341c36f48d0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (100x200 and 400x200)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-cefbdd5777cb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-2b48cbf6c643>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model, train_loader, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mBCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKLD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mKLD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-8a0bfe9c99c1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreparameterization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-5ef8a966ee26>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mh_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mh_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mlog_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[0;31m#인코더에서 평균과 분산 반환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x200 and 400x200)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#텐서보드에서 오차 확인\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir scalar --port=6013"
      ],
      "metadata": {
        "id": "QKMGaG2AoaGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13.3 적대적 생성 신경망(Generative Adversarial Network, GAN)\n",
        "\n",
        "- 판별자 D(Discriminator) : 주어진 입력 이미지가 진짜 이미지인지 가짜 이미지인지 구별.\n",
        "\n",
        "  즉, 입력 이미지 x에 대하여 D(x)가 진짜 이미지일 확률 반환\n",
        "\n",
        "- 생성자 G(Generator) : 실제 이미지를 학습해 노이즈 데이터를 사용하여 구분하기 어려운 모조 이미지를 생성\n",
        "\n",
        "  즉, 진짜 이미지 z에 대하여 판별자가 G(z)를 입력으로 받아 D(G(z))를 1로 예측하게 하는 것이 목표"
      ],
      "metadata": {
        "id": "6A62eny3q6w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#라이브러리 호출\n",
        "import imageio\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "from torchvision.utils import make_grid, save_image\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "matplotlib.style.use('ggplot')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "2hgb2gFFsrVL"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#변수 값 설정\n",
        "batch_size = 512\n",
        "epochs = 200\n",
        "sample_size = 64 #생성자에 제공할 고정 크기 노이즈 벡터 크기\n",
        "nz = 128 #잠재 벡터 크기(생성자의 입력 크기와 동일)\n",
        "k = 1 #판별자에 적용할 스텝 수"
      ],
      "metadata": {
        "id": "sRpDg52htPWX"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#생성자 네트워크 생성\n",
        "class Generator(nn.Module) :\n",
        "  def __init__(self, nz) :\n",
        "    super(Generator, self).__init__()\n",
        "    self.nz = nz\n",
        "    self.main = nn.Sequential(\n",
        "        nn.Linear(self.nz, 256),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Linear(256, 512),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Linear(512, 1024),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Linear(1024, 784),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  def forward(self, x) :\n",
        "    return self.main(x).view(-1, 1, 28, 28) #생성자 네트워크의 반환값 : '배치 크기x1x28x28'"
      ],
      "metadata": {
        "id": "p9_hZIdvupZX"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#판별자 네트워크 생성 (이진 분류)\n",
        "\n",
        "class Discriminator(nn.Module) :\n",
        "  def __init__(self) :\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.n_input = 784\n",
        "    self.main = nn.Sequential(\n",
        "        nn.Linear(self.n_input, 1024),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(1024, 512),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(512, 256),\n",
        "        nn.LeakyReLU(0.2),\n",
        "        nn.Dropout(0.3),\n",
        "        nn.Linear(256, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  def forward(self, x) :\n",
        "    x = x.view(-1, 784)\n",
        "    return self.main(x) #이미지가 진짜인지 가짜인지 분류하는 값 반환"
      ],
      "metadata": {
        "id": "zJA3TAMMvbsw"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#생성자와 판별자 네트워크 초기화\n",
        "generator = Generator(nz).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "print(generator)\n",
        "print(discriminator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "760A1bZTwTZ4",
        "outputId": "960f8ad2-96ec-4b94-a820-f8d1d6381528"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.2)\n",
            "    (4): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.2)\n",
            "    (6): Linear(in_features=1024, out_features=784, bias=True)\n",
            "    (7): Tanh()\n",
            "  )\n",
            ")\n",
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Dropout(p=0.3, inplace=False)\n",
            "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2)\n",
            "    (5): Dropout(p=0.3, inplace=False)\n",
            "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Dropout(p=0.3, inplace=False)\n",
            "    (9): Linear(in_features=256, out_features=1, bias=True)\n",
            "    (10): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#옵티마이저와 손실 함수 정의 : 생성자와 판별자에서 사용할 옵티마이저를 >따로< 정의해야 함\n",
        "optim_g = optim.Adam(generator.parameters(), lr=0.0002) #생성자의 옵티마이저\n",
        "optim_d = optim.Adam(discriminator.parameters(), lr=0.0002) #판별자의 옵티마이저\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "losses_g = [] #에포크마다 생성자 오차 저장 리스트\n",
        "losses_d = [] #에포크마다 판별자 오차 저장 리스트\n",
        "images = [] #생성자가 생성하는 이미지 저장 리스트"
      ],
      "metadata": {
        "id": "h9s4usHOwo3d"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#생성한 이미지 저장 함수\n",
        "def save_generator_image(image, path) :\n",
        "  save_image(image, path)"
      ],
      "metadata": {
        "id": "ewNeLHx2xoIR"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#판별자 학습을 위한 함수\n",
        "def train_discriminator(optimizer, data_real, data_fake) :\n",
        "  b_size = data_real.size(0)\n",
        "  real_label = torch.ones(b_size, 1).to(device) #전체 데이터에 대한 레이블 정보 텐서 (진짜이므로 레이블이 1)\n",
        "  fake_label = torch.zeros(b_size, 1).to(device) #가짜 데이터에 대한 레이블 정보 텐서 (가짜이므로 레이블이 0)\n",
        "  optimizer.zero_grad()\n",
        "  output_real = discriminator(data_real)\n",
        "  loss_real = criterion(output_real, real_label)\n",
        "  output_fake = discriminator(data_fake)\n",
        "  loss_fake = criterion(output_fake, fake_label)\n",
        "  loss_real.backward()\n",
        "  loss_fake.backward()\n",
        "  optimizer.step()\n",
        "  return loss_real + loss_fake"
      ],
      "metadata": {
        "id": "ASzAYHL1x_l5"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#생성자 학습 위한 함수\n",
        "def train_generator(optimizer, data_fake) :\n",
        "  b_size = data_fake.size(0)\n",
        "  real_label = torch.ones(b_size, 1).to(device)\n",
        "  optimizer.zero_grad()\n",
        "  output = discriminator(data_fake)\n",
        "  loss = criterion(output, real_label)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss"
      ],
      "metadata": {
        "id": "HWIh8nPzzMYn"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델 학습\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "\n",
        "for epoch in range(epochs) :\n",
        "  loss_g = 0.0\n",
        "  loss_d = 0.0\n",
        "  for idx, data in tqdm(enumerate(train_loader), total=int(len(train_dataset)//train_loader.batch_size)) :\n",
        "    image, _ = data\n",
        "    image = image.to(device)\n",
        "    b_size = len(image)\n",
        "    for step in range(k) :\n",
        "      data_fake = generator(torch.randn(b_size, nz).to(device)).detach()\n",
        "      data_real = image\n",
        "      loss_d += train_discriminator(optim_d, data_real, data_fake)\n",
        "    data_fake = generator(torch.randn(b_size, nz).to(device))\n",
        "    loss_g += train_generator(optim_g, data_fake)\n",
        "  generated_img = generator(torch.randn(b_size, nz).to(device)).cpu().detach()\n",
        "  generated_img = make_grid(generated_img)\n",
        "  save_generator_image(generated_img, f'/content/drive/MyDrive/pytorch_ex/chap13/data/gen_img{epoch}.png')\n",
        "  images.append(generated_img)\n",
        "  epoch_loss_g = loss_g / idx\n",
        "  epoch_loss_d = loss_d / idx\n",
        "  losses_g.append(epoch_loss_g)\n",
        "  losses_d.append(epoch_loss_d)\n",
        "\n",
        "  print(f'Epoch {epoch} of {epochs}')\n",
        "  print(f'Generator Loss: {epoch_loss_g:.8f}, Discriminator loss: {epoch_loss_d:.8f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI3Kt7EEzuQa",
        "outputId": "cca4b353-0c4f-4b8d-80ba-121e4e782263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100%|██████████| 600/600 [01:15<00:00,  7.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 of 200\n",
            "Generator Loss: 2.60532069, Discriminator loss: 0.68645126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:12<00:00,  8.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 200\n",
            "Generator Loss: 2.55686641, Discriminator loss: 0.63636541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:14<00:00,  8.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 of 200\n",
            "Generator Loss: 2.38371110, Discriminator loss: 0.67563313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:15<00:00,  7.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 of 200\n",
            "Generator Loss: 2.39061356, Discriminator loss: 0.68801290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:13<00:00,  8.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 of 200\n",
            "Generator Loss: 2.02452350, Discriminator loss: 0.76222873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:11<00:00,  8.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 of 200\n",
            "Generator Loss: 2.05510187, Discriminator loss: 0.73811656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:14<00:00,  8.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 of 200\n",
            "Generator Loss: 2.22369003, Discriminator loss: 0.67843074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:13<00:00,  8.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 of 200\n",
            "Generator Loss: 2.14266133, Discriminator loss: 0.75003839\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:10<00:00,  8.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 of 200\n",
            "Generator Loss: 2.17580152, Discriminator loss: 0.73809457\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:11<00:00,  8.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 of 200\n",
            "Generator Loss: 2.44037986, Discriminator loss: 0.72306448\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:08<00:00,  8.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 of 200\n",
            "Generator Loss: 2.15488577, Discriminator loss: 0.72204858\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:09<00:00,  8.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 of 200\n",
            "Generator Loss: 2.24844050, Discriminator loss: 0.68287253\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:09<00:00,  8.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 of 200\n",
            "Generator Loss: 2.17541599, Discriminator loss: 0.68671006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:06<00:00,  8.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 of 200\n",
            "Generator Loss: 2.26839709, Discriminator loss: 0.68150461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:07<00:00,  8.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 of 200\n",
            "Generator Loss: 2.21292901, Discriminator loss: 0.69587868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:06<00:00,  9.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 of 200\n",
            "Generator Loss: 2.54383588, Discriminator loss: 0.67280155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:07<00:00,  8.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 of 200\n",
            "Generator Loss: 2.27471352, Discriminator loss: 0.63874590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:09<00:00,  8.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 of 200\n",
            "Generator Loss: 2.32367110, Discriminator loss: 0.62485367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:07<00:00,  8.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 of 200\n",
            "Generator Loss: 2.34178114, Discriminator loss: 0.62111485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:14<00:00,  8.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 of 200\n",
            "Generator Loss: 2.45393610, Discriminator loss: 0.63057005\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 600/600 [01:11<00:00,  8.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 of 200\n",
            "Generator Loss: 2.48742867, Discriminator loss: 0.62563401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            " 82%|████████▏ | 490/600 [00:57<00:12,  8.52it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#생성자와 판별자의 오차 확인\n",
        "plt.figure()\n",
        "losses_g = [f1.item() for f1 in losses_g]\n",
        "plt.plot(losses_g, label='Generator loss')\n",
        "loss_d = [f2.item() for f2 in losses_d]\n",
        "plt.plot(losses_d, label='Discriminator loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "0nOYyE_k2ES_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#생성된 이미지 출력\n",
        "fake_images = generator(torch.randn(b_size, nz).to(device))\n",
        "for i in range(10) :\n",
        "  fake_images_img = np.reshape(fake_images.data.cpu().numpy()[i],(28,28))\n",
        "  plt.imshow(fake_images_img, cmap='gray')\n",
        "  plt.savefig('/content/drive/MyDrive/pytorch_ex/chap13/data/fake_images_img' + str(i) + '.png')"
      ],
      "metadata": {
        "id": "D1zo8noR36NN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}