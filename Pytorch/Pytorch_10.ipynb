{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **10장. 자연어 처리를 위한 임베딩**\n",
        "\n",
        "임베딩 : 자연어(사람이 사용하는 언어) -> 벡터(숫자, 컴퓨터가 이해할 수 있는 언어) 형태로 변환하는 결과 또는 과정\n",
        "\n",
        "임베딩의 역할\n",
        "\n",
        "- 단어 및 문장 간 관련성 계산\n",
        "- 의미적 혹은 문법적 정보 함축(ex: 왕-여왕, 교사-학생)"
      ],
      "metadata": {
        "id": "ncHd9bJlDaEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.1.1 희소 표현 기반 임베딩**\n",
        "\n",
        "희소 표현(sparse representation) : 대부분의 값이 0으로 채워져 있는 경우.\n",
        "\n",
        "- 원 핫 인코딩(one-hot encoding) : 단어 N개를 각각 N차원 벡터로 표현하는 방식, 단어가 포함되는 위치에 1을 넣고 나머지를 0으로 채움"
      ],
      "metadata": {
        "id": "vy8lNgmpD4hr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF9n3jbzDZKm",
        "outputId": "1cb25cb8-afeb-4b40-eba4-8d1094cd1710"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#원-핫 인코딩 적용\n",
        "import pandas as pd\n",
        "class2 = pd.read_csv('/content/drive/MyDrive/pytorch_ex/chap10/data/class2.csv')\n",
        "\n",
        "from sklearn import preprocessing\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "onehot_encoder = preprocessing.OneHotEncoder()\n",
        "\n",
        "train_x = label_encoder.fit_transform(class2['class2'])\n",
        "train_x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.1.2 횟수 기반 임베딩**\n",
        "\n",
        "횟수 기반 : 단어가 출현할 빈도를 고려하여 임베딩하는 방법\n",
        "\n",
        "- 카운터 벡터(counter vector) : 문서 집합에서 단어를 토큰으로 생성, 각 단어의 출현 빈도수를 이용하여 인코딩해 벡터를 만드는 방법\n",
        "\n",
        "  즉, 토크나이징과 벡터화가 동시에 가능한 방법.\n",
        "\n",
        "- TF-IDF(Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "  TF: 한 문서 내에서 특정 단어의 출현 빈도 -> 높을수록 해당 문서와 단어의 관련이 높음\n",
        "\n",
        "  DF: 전체 문서 중 특정 단어가 포함된 문서의 개수 -> 높을수록 일반적인 단어로 간주되어 가중치를 낮추어줘야 함\n",
        "\n",
        "  IDF: DF값이 클수록 가중치값을 낮춰주기 위해 DF 값에 역수를 취한 것"
      ],
      "metadata": {
        "id": "Q-hXo83iINMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#코퍼스(말뭉치)에 카운터 벡터 적용\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is last chance.',\n",
        "    'and if you do not have this chance',\n",
        "    'you will never get any chance',\n",
        "    'will you do get this one?',\n",
        "    'please, get this chance'\n",
        "]\n",
        "vect = CountVectorizer()\n",
        "vect.fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlzuL5e_HQ-n",
        "outputId": "a9fe6b54-8ba1-4ce4-86dc-0068118aba1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 13,\n",
              " 'is': 7,\n",
              " 'last': 8,\n",
              " 'chance': 2,\n",
              " 'and': 0,\n",
              " 'if': 6,\n",
              " 'you': 15,\n",
              " 'do': 3,\n",
              " 'not': 10,\n",
              " 'have': 5,\n",
              " 'will': 14,\n",
              " 'never': 9,\n",
              " 'get': 4,\n",
              " 'any': 1,\n",
              " 'one': 11,\n",
              " 'please': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#적용 결과를 배열로 변환\n",
        "vect.transform(['you will never get any chance']).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01VD8CvCJ1Vr",
        "outputId": "3bee465f-d654-4e78-dab2-b1a1476f6bf8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#불용어를 제거한 카운터 벡터\n",
        "vect = CountVectorizer(stop_words=['and', 'is', 'please', 'this']).fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JXcqJy2J-qb",
        "outputId": "7b265558-6df3-463e-d35c-1e8f4e9b8034"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'last': 6,\n",
              " 'chance': 1,\n",
              " 'if': 5,\n",
              " 'you': 11,\n",
              " 'do': 2,\n",
              " 'not': 8,\n",
              " 'have': 4,\n",
              " 'will': 10,\n",
              " 'never': 7,\n",
              " 'get': 3,\n",
              " 'any': 0,\n",
              " 'one': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF 적용 후 행렬로 표현\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "doc = [ 'I like machine learning', 'I ove deep learning', 'I run everyday' ]\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(doc)\n",
        "doc_distance = (tfidf_matrix * tfidf_matrix.T)\n",
        "print('유사도를 위한', str(doc_distance.get_shape()[0]), 'x', str(doc_distance.get_shape()[1]), '행렬을 만들었습니다')\n",
        "print(doc_distance.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrIb9V4hKMNJ",
        "outputId": "e2f7ebd3-c6da-4029-a2e4-c8efc7aefa45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "유사도를 위한 3 x 3 행렬을 만들었습니다\n",
            "[[1.       0.224325 0.      ]\n",
            " [0.224325 1.       0.      ]\n",
            " [0.       0.       1.      ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.1.3 예측 기반 임베딩**\n",
        "\n",
        ": 신경망 구조나 모델을 이용하여, 특정 문맥에서 어떤 단어가 나올지 예측하면서 단어를 벡터로 만드는 방식\n",
        "\n",
        "- 워드투벡터(Word2Vec) : 신경망 알고리즘. 텍스트의 각 단어를 하나의 일련의 벡터로 출력하되, 의미론적으로 유사한 단어의 벡터를 서로 가깝게 표현\n",
        "- CBOW(Continuous Bag Of Words) : 주변 단어에서 중심 단어 예측\n",
        "- skip-gram : 중심 단어에서 주변 단어 예측"
      ],
      "metadata": {
        "id": "ckEbHT7YOXwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDVL9mFJT6fm",
        "outputId": "c1816760-f142-440e-b034-40b4cef07bb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋을 메모리로 로딩, 토큰화 적용\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sample = open('/content/drive/MyDrive/pytorch_ex/chap10/data/peter.txt', 'r', encoding='UTF8')\n",
        "s = sample.read()\n",
        "\n",
        "f = s.replace('\\n', ' ') #줄바꿈 -> 공백으로 변환\n",
        "data = []\n",
        "\n",
        "for i in sent_tokenize(f) : #각 문장마다~\n",
        "  temp =[]\n",
        "  for j in word_tokenize(i) : #문장을 단어로 토큰화하여, 각 단어마다~\n",
        "      temp.append(j.lower()) #소문자로 변환하여 저장\n",
        "  data.append(temp)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "Ro98tUVKOLdP",
        "outputId": "01226b23-8083-4b33-d65f-01c1317714ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-95967196abf5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10.2 트랜스포머 어텐션\n",
        "\n",
        "**10.2.1 seq2seq**"
      ],
      "metadata": {
        "id": "lqNzFjYTrXaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#영어를 프랑스어로 번역하는 예제 - seq2seq 구현\n",
        "\n",
        "#라이브러리 호출\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "\n",
        "import re\n",
        "import random\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "37UQL5mkrn25"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 준비\n",
        "SOS_token = 0   #Start Of Sequence, 문장의 시작\n",
        "EOS_token = 1   #End Of Sequence, 문장의 끝\n",
        "MAX_LENGTH = 20\n",
        "\n",
        "class Lang :\n",
        "  def __init__(self) :\n",
        "    self.word2index = {}\n",
        "    self.word2count = {}\n",
        "    self.index2word = {0:'SOS', 1:'EOS'}\n",
        "    self.n_words = 2\n",
        "\n",
        "  def add2Sentence(self, sentence) :\n",
        "    for word in sentence.split(' ') :\n",
        "      self.addWord(word)\n",
        "\n",
        "  def addWord(self, word) :\n",
        "    if word not in self.word2index :\n",
        "      self.word2index[word] = self.n_words\n",
        "      self.word2count[word] = 1\n",
        "      self.index2word[self.n_words] = word\n",
        "      self.n_words += 1\n",
        "    else :\n",
        "      self.word2count[word] += 1"
      ],
      "metadata": {
        "id": "GWc6_p1Gskpi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터세트 정규화: 영어와 프랑스어가 탭(tab)으로 구성된 text 파일 -> pandas로 불러와서 정규화\n",
        "\n",
        "def normalizeString(df, lang) :\n",
        "  sentence = df[lang].str.lower() #소문자로 전환\n",
        "  sentence = sentence.str.replace('[A-Za-z\\s]+', ' ') #A-Z, a-z, ..., !, ? 등을 제외하고 전부 공백으로 바꿈\n",
        "  sentence = sentence.str.normalize('NFD') #유니코드 정규화 방식\n",
        "  sentence = sentence.str.encoding('ascii', errors='ignore').str.decode(UTF8)  #####encode->encoding이라고 고침\n",
        "  return sentence\n",
        "\n",
        "def read_sentence(df, lang1, lang2) :\n",
        "  sentence1 = normalizeString(df, lang1) #데이터세트의 첫번쨰 열(영어)\n",
        "  sentence2 = normalizeString(df, lang2) #데이터세트의 두번째 열(프랑스어)\n",
        "  return sentence1, sentence2\n",
        "\n",
        "def read_file(loc, lang1, lang2) :\n",
        "  df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2])\n",
        "  return df\n",
        "\n",
        "def process_data(lang1, lang2) :\n",
        "  df = read_file('/content/drive/MyDrive/pytorch_ex/chap10/data/%s-%s.txt' % (lang1, lang2), lang1, lang2)\n",
        "  sentence1, sentence2 - read_sentence(df, lang1, lang2)\n",
        "\n",
        "  input_lang = Lnag()\n",
        "  output_lang = Lang()\n",
        "  pairs = []\n",
        "  for i in range(len(df)) :\n",
        "    if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH :\n",
        "      full = [sentence1[i], sentence2[i]]\n",
        "      input_lang.addSentence(sentence1[i])\n",
        "      output_lang.addSentence(sentence2[i])\n",
        "      pairs.appdne(full)\n",
        "\n",
        "  return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "W3g-a8h1v0mo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#텐서로 변환\n",
        "def indexesFromSentence(lang, sentence) : #문장을 단어로 분리하여 그 인덱스를 반환\n",
        "  return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence) :\n",
        "  indexes = indexesFromSentence(lang, sentence) #딕셔너리에서 단어에 대한 인덱스 가져옴\n",
        "  indexes.append(EOS_token) #문장 끝에 EOS 토큰 추가\n",
        "  return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(input_lang, output_lang, pair) : #입력과 출력 문장을 텐서로 변환 후 반환\n",
        "  input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "  target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "  return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "yfYsM1fu_pYX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#인코더 네트워크\n",
        "class Encoder(nn.Module) :\n",
        "  def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers) :\n",
        "    super(Encoder, self).__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.embbed_dim = embbed_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n",
        "    self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
        "\n",
        "  def forward(self, src) :\n",
        "    embedded = self.embedding(src).view(1, 1, -1) #임베딩 처리\n",
        "    outputs, hidden = self.gru(embedded) #임베딩 결과를 GRU 모델에 적용\n",
        "    return outputs, hidden"
      ],
      "metadata": {
        "id": "KGnJECVBD8iH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#디코더 네트워크\n",
        "class Decoder(nn.Module) :\n",
        "  def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers) :\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.embbed_dim = embbed_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.output_dim = output_dim\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = nn.Embedding(output_dim, self.embbed_dim) #임베딩 계층 초기화\n",
        "    self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers) #GRU 계층 초기화\n",
        "    self.out = nn.Linear(self.hidden_dim, output_dim)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, input, hidden) :\n",
        "    input = input.view(1, -1) #입력을 (1, 배치크기)로 변경\n",
        "    embedded = self.embedding(input) #임베딩\n",
        "    embedded = F.relu(embedded) #렐루 함수\n",
        "    output, hidden = self.gru(embedded, hidden) #GRU 적용 -> 출력, 은닉상태\n",
        "    prediction = self.out(output[0]) #출력만 선형층 통과\n",
        "    prediction = self.softmax(prediction) #소프트맥스 적용 -> 예측 출력\n",
        "    return prediction, hidden"
      ],
      "metadata": {
        "id": "KciJwN3UL5-a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#seq2seq 네트워크\n",
        "\n",
        "class Seq2seq(nn.Module) :\n",
        "  def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH) :\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "  def forward(self, input_lang, output_lang, teacher_forcing_ratio=0.5) :\n",
        "    input_length = input_lang.size(0) #입력 문자 길이(문장의 단어 수)\n",
        "    batch_size = output_lang.shape[1]\n",
        "    target_length = output_lang.shape[0]\n",
        "    vocab_size = self.decoder.output_dim\n",
        "    outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "    for i in range(input_length) :\n",
        "      encoder_output, encoder_hidden = self.encoder(input_lang[i]) #문장의 모든 단어 인코딩\n",
        "    decoder_hidden = encoder_hidden.to(device)\n",
        "    decoder_input = torch.tensor([SOS_token], device=device)\n",
        "\n",
        "    for t in range(target_length) :\n",
        "      decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
        "      outputs[t] = decoder_output\n",
        "      teacher_force = random.random() < teacher_forcing_ratio\n",
        "      topv, topi = decoder_output.topk(1)\n",
        "      input = (output_lang[t] if teacher_force else topi)\n",
        "      if (teacher_force == False and input.item() == EOS_token) :\n",
        "        break\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "ZR7EnbVBS5ET"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}