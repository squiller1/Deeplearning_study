{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **10장. 자연어 처리를 위한 임베딩**\n",
        "\n",
        "임베딩 : 자연어(사람이 사용하는 언어) -> 벡터(숫자, 컴퓨터가 이해할 수 있는 언어) 형태로 변환하는 결과 또는 과정\n",
        "\n",
        "임베딩의 역할\n",
        "\n",
        "- 단어 및 문장 간 관련성 계산\n",
        "- 의미적 혹은 문법적 정보 함축(ex: 왕-여왕, 교사-학생)"
      ],
      "metadata": {
        "id": "ncHd9bJlDaEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.1.1 희소 표현 기반 임베딩**\n",
        "\n",
        "희소 표현(sparse representation) : 대부분의 값이 0으로 채워져 있는 경우.\n",
        "\n",
        "- 원 핫 인코딩(one-hot encoding) : 단어 N개를 각각 N차원 벡터로 표현하는 방식, 단어가 포함되는 위치에 1을 넣고 나머지를 0으로 채움"
      ],
      "metadata": {
        "id": "vy8lNgmpD4hr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF9n3jbzDZKm",
        "outputId": "1cb25cb8-afeb-4b40-eba4-8d1094cd1710"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#원-핫 인코딩 적용\n",
        "import pandas as pd\n",
        "class2 = pd.read_csv('/content/drive/MyDrive/pytorch_ex/chap10/data/class2.csv')\n",
        "\n",
        "from sklearn import preprocessing\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "onehot_encoder = preprocessing.OneHotEncoder()\n",
        "\n",
        "train_x = label_encoder.fit_transform(class2['class2'])\n",
        "train_x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.1.2 횟수 기반 임베딩**\n",
        "\n",
        "횟수 기반 : 단어가 출현할 빈도를 고려하여 임베딩하는 방법\n",
        "\n",
        "- 카운터 벡터(counter vector) : 문서 집합에서 단어를 토큰으로 생성, 각 단어의 출현 빈도수를 이용하여 인코딩해 벡터를 만드는 방법\n",
        "\n",
        "  즉, 토크나이징과 벡터화가 동시에 가능한 방법.\n",
        "\n",
        "- TF-IDF(Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "  TF: 한 문서 내에서 특정 단어의 출현 빈도 -> 높을수록 해당 문서와 단어의 관련이 높음\n",
        "\n",
        "  DF: 전체 문서 중 특정 단어가 포함된 문서의 개수 -> 높을수록 일반적인 단어로 간주되어 가중치를 낮추어줘야 함\n",
        "\n",
        "  IDF: DF값이 클수록 가중치값을 낮춰주기 위해 DF 값에 역수를 취한 것"
      ],
      "metadata": {
        "id": "Q-hXo83iINMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#코퍼스(말뭉치)에 카운터 벡터 적용\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is last chance.',\n",
        "    'and if you do not have this chance',\n",
        "    'you will never get any chance',\n",
        "    'will you do get this one?',\n",
        "    'please, get this chance'\n",
        "]\n",
        "vect = CountVectorizer()\n",
        "vect.fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlzuL5e_HQ-n",
        "outputId": "a9fe6b54-8ba1-4ce4-86dc-0068118aba1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 13,\n",
              " 'is': 7,\n",
              " 'last': 8,\n",
              " 'chance': 2,\n",
              " 'and': 0,\n",
              " 'if': 6,\n",
              " 'you': 15,\n",
              " 'do': 3,\n",
              " 'not': 10,\n",
              " 'have': 5,\n",
              " 'will': 14,\n",
              " 'never': 9,\n",
              " 'get': 4,\n",
              " 'any': 1,\n",
              " 'one': 11,\n",
              " 'please': 12}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#적용 결과를 배열로 변환\n",
        "vect.transform(['you will never get any chance']).toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01VD8CvCJ1Vr",
        "outputId": "3bee465f-d654-4e78-dab2-b1a1476f6bf8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#불용어를 제거한 카운터 벡터\n",
        "vect = CountVectorizer(stop_words=['and', 'is', 'please', 'this']).fit(corpus)\n",
        "vect.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JXcqJy2J-qb",
        "outputId": "7b265558-6df3-463e-d35c-1e8f4e9b8034"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'last': 6,\n",
              " 'chance': 1,\n",
              " 'if': 5,\n",
              " 'you': 11,\n",
              " 'do': 2,\n",
              " 'not': 8,\n",
              " 'have': 4,\n",
              " 'will': 10,\n",
              " 'never': 7,\n",
              " 'get': 3,\n",
              " 'any': 0,\n",
              " 'one': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF 적용 후 행렬로 표현\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "doc = [ 'I like machine learning', 'I ove deep learning', 'I run everyday' ]\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(doc)\n",
        "doc_distance = (tfidf_matrix * tfidf_matrix.T)\n",
        "print('유사도를 위한', str(doc_distance.get_shape()[0]), 'x', str(doc_distance.get_shape()[1]), '행렬을 만들었습니다')\n",
        "print(doc_distance.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrIb9V4hKMNJ",
        "outputId": "e2f7ebd3-c6da-4029-a2e4-c8efc7aefa45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "유사도를 위한 3 x 3 행렬을 만들었습니다\n",
            "[[1.       0.224325 0.      ]\n",
            " [0.224325 1.       0.      ]\n",
            " [0.       0.       1.      ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10.1.3 예측 기반 임베딩**\n",
        "\n",
        ": 신경망 구조나 모델을 이용하여, 특정 문맥에서 어떤 단어가 나올지 예측하면서 단어를 벡터로 만드는 방식\n",
        "\n",
        "- 워드투벡터(Word2Vec) : 신경망 알고리즘. 텍스트의 각 단어를 하나의 일련의 벡터로 출력하되, 의미론적으로 유사한 단어의 벡터를 서로 가깝게 표현\n",
        "- CBOW(Continuous Bag Of Words) : 주변 단어에서 중심 단어 예측\n",
        "- skip-gram : 중심 단어에서 주변 단어 예측"
      ],
      "metadata": {
        "id": "ckEbHT7YOXwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDVL9mFJT6fm",
        "outputId": "c1816760-f142-440e-b034-40b4cef07bb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터셋을 메모리로 로딩, 토큰화 적용\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sample = open('/content/drive/MyDrive/pytorch_ex/chap10/data/peter.txt', 'r', encoding='UTF8')\n",
        "s = sample.read()\n",
        "\n",
        "f = s.replace('\\n', ' ') #줄바꿈 -> 공백으로 변환\n",
        "data = []\n",
        "\n",
        "for i in sent_tokenize(f) : #각 문장마다~\n",
        "  temp =[]\n",
        "  for j in word_tokenize(i) : #문장을 단어로 토큰화하여, 각 단어마다~\n",
        "      temp.append(j.lower()) #소문자로 변환하여 저장\n",
        "  data.append(temp)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "Ro98tUVKOLdP",
        "outputId": "01226b23-8083-4b33-d65f-01c1317714ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-95967196abf5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0;31m# try to load fast, cythonized code if possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_difference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirichlet_expectation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/_matutils.pyx\u001b[0m in \u001b[0;36minit gensim._matutils\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    }
  ]
}